# Inference endpoint (OpenAI-compatible)
BASE_URL=https://your-openai-compatible-endpoint.com

# Authentication (choose one path)
# 1) API key (OpenAI-style)
INFERENCE_API_KEY=your-api-key

# 2) Keycloak client credentials (if your endpoint is protected this way)
KEYCLOAK_CLIENT_ID=api
KEYCLOAK_CLIENT_SECRET=your-client-secret
KEYCLOAK_REALM=master

# Model configuration
INFERENCE_MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct

# LLM Settings
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=4096

# Code Translation Settings
MAX_CODE_LENGTH=10000
MAX_FILE_SIZE=10485760

# CORS Configuration
CORS_ALLOW_ORIGINS=http://localhost:5173,http://localhost:3000
